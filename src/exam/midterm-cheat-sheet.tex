%! Author = Len Washington III
%! Date = 2/20/24

% Preamble
\documentclass[exam={Midterm},color=true]{cs581exam}

\defineexamcolor{nodeblue}{00b0f0}
\defineexamcolor{frontiergreen}{00b050}
\defineexamcolor{reachedpurple}{8d5ab3}
\defineexamcolor{childred}{c00000}
\defineexamcolor{stateorange}{ffc000}

\setcounter{section}{1}

% Document
\begin{document}

\section{Intelligent Agents}\label{sec:intelligent-agents}
\subsection{Agents and Environments}\label{subsec:2.1}
\definition{Percept}{content/information that agent's sensors are perceiving / capturing currently}.
\definition{Percept Sequence}{a complete history of everything that agent has ever perceived}.
Action $<$-$>$ percept sequence \emph{mapping} IS the agent \emph{function}.
Agent \emph{function} describes agent \emph{behavior}.
Agent \emph{function} is an \emph{abstract concept}.
Agent \emph{program} implements agent \emph{function}.

\subsection{The Concept of Rationality}\label{subsec:2.2}
A \emph{rational agent} is one that acts to achieve the best outcome, or when there is uncertainty, the best expected outcome.
Rationality maximizes what is EXPECTED to happen.
Perfection maximizes what WILL happen.
%
\subsection{The Nature of Environments}\label{subsec:2.3}
PEAS -- \Peas{}erformance measure;
\pEas{}nvironment in which the agent will operate;
\peAs{}ctuators that the agent will use to affect the environment;
\peaS{}ensors.
\subsubsection{Properties}\label{subsubsec:properties}
Fully vs partially observable.
Single agent vs multiagent (comp vs. co-op).
Deterministic vs. nondeterministic (stochastic/random).
Episode vs. sequential.
Static vs. dynamic.
Discrete vs. continuous.
Known vs. unknown (to the agent).

Deterministic means that the next state is \emph{completely determined} by the current state and agent action
whereas nondeterministic is \emph{NOT completely determined} by the current state and agent action.
Deterministic AND fully observable environment means there's no uncertainty, deterministic AND partially observable \textit{may} appear nondeterministic.

Episodic means that agent experience is divided into individual, \emph{independent}, and atomic episodes (one percept per action),
which means the next action is not a function of the previous function and doesn't need to be remembered.
Sequential means that the current decision \emph{COULD} affect all future decisions/actions.

Static environments CANNOT change while the agent is taking its time deciding while dynamic environments CAN change and speed if important.

Agents in known environments know all outcomes (and probabilities) to its actions, essentially, the agent knows how the environment works.
An unknown environment means the agent doesn't know all the details about the inner workings of the environment and some \emph{learning and exploration} can be necessary.
%
\subsection{The Structure of Agents}\label{subsec:2.4}
Agent = Architecture + Program
Simple-reflex agents use condition-action rules;
Model-based reflex agents keep track of the unobserved parts of the environment by maintaining an internal state;
Goal-based reflex agents maintain the model of the world and goals to select decisions (that lead to the goal);
Utility-based reflex agents maintain the model of the world and utility function to select PREFERRED decisions (that lead to the best expected utility: \Call{AVG}{EU*$p$}).

\subsubsection{State and Transition Representations}\label{subsubsec:state-and-transition-representations}
Atomic state representations have NO internal structure;
Factored state representation includes fixed attributes (which can have values);
and Structured states include objects and their relationships.

%
\subsection{Summary}\label{subsec:2-summary}
Go through the chapter summary.

\section{Solving Problems by Search}\label{sec:solving-problems-by-search}
\subsection{Problem-Solving Agents}\label{subsec:3.1}
Be comfortable defining a search problem.
\subsection{Example Problems}\label{subsec:3.2}
\subsection{Search Algorithms \& Uniform Search Strategies}\label{subsec:3.3-4}
Unlike A*, Greedy Best First Search is not always going to find the optimal solution, even with admissible heuristics.
It is complete, \emph{not optimal}, but often efficient.

Ignore sections 3.4.4 and 3.4.5 for the exam.
\setcounter{subsection}{4}%
\subsection{Informed (Heuristic) Search Strategies}\label{subsec:3.5}
Informed search relies on \emph{domain-specific knowledge / hints} that help locate the goal state.
$h(n)=h(\mbox{State n})=\mbox{relevant information about State } n$

You may be asked to solve a search problem by hand.
\subsection{Heuristic Functions}\label{subsec:3.6}
Straight-line heuristics is \emph{admissible}: it never overestimates the cost.
An \emph{admissible heuristics} is guaranteed to give you the optimal solution.
Every \emph{consistent} heuristics is \emph{admissible}, but not the other way around.
A \emph{dominating heuristic} is a heuristic that estimates closer to the actual cost than another heuristic,
with the offset that dominating heuristics are more costly to compute.

\subsection{A*}\label{subsec:a*}
The evaluation function for A* is $f(n) = g(\mbox{State}_{n}) + h(\mbox{State}_{n})$ where $g(n)$ is the path cost from the initial node to node $n$,
and $h(n)$ is the \emph{estimated cost} of the best path that continues from node $n$ to a goal node.
A node $n$ with minimum (or maximum if necessary) $f(n)$ should be chosen for expansion.
\subsection{Summary}\label{subsec:3-summary}
Go through the chapter summary. FOCUS ON A* algorithm

\section{Search in Complex Environments}\label{sec:search-in-complex-environments}
\subsection{Local Search and Optimization Problems}\label{subsec:local-search-and-optimization-problems}
Local search doesn't care about the path to the goal, just getting to the goal.
They're useful for pure optimization problems (finding the best state according to an objective function.)
Generally use a single current state and generally move to neighbors of that state.
Two key advantages are: little memory usage (usually a constant amount) and can find reasonable solutions in large of infinite (continuous) states spaces.
The performance can be measured using \emph{completeness} (guaranteed to find a solution when there is one and report when there isn't), \emph{cost-optimality} (does it find a solution with the lowest path cost of all solutions), or time or space complexity.
%
\subsubsection{Hill-climbing search}\label{subsubsec:4.1.1}
The most primitive informed search approach; it is a naive greedy algorithm and the objective function is the value of the next state.
The agent can get stuck on peaks (local maxima), ridges (sequences of peaks), and plateaus (areas where the evaluation function has the same value).

\subsubsection{Simulated Annealing}\label{subsubsec:4.1.2}
Accepts a move if it improves the objective value, and accepts some ``bad'' moves given some probability depending on the current objective value.
Converges to a global optimum; in practice, it can give excellent results.

\setcounter{subsection}{3}%
\subsubsection{Evolutionary algorithms}\label{subsubsec:4.1.4}
\definition{Nature}{speciation occurs when two similar reproducing beings evolve to become too dissimilar to share genetic information effectively or correctly}.
\definition{Implementation}{speciation$\rightarrow$some mathematical function that established the similarity between two candidate solutions in the population}

$\dots$and everything related to Evolutionary algorithms that I covered in class (especially: EVERYTHING about GENETIC ALGORITHM)\\
IGNORE TABU SEARCH

\subsubsection{Genetic Programming (GP)}\label{subsubsec:genetic-programming}
GP is an automated method for creating a working computer program from a high-level problem statement of a problem.
It starts from a high-level statement of ``what needs to be done'' and automatically creates a computer program to solve the problem.
Genotypes are trees, whereas the phenotypes (what can be seen) is the evaluation score.
Mutation occurs by picking a node for random mutation (by generating a new random subtree) and replacing the node with the root of the new subtree.
Crossover occurs by swapping the values of two nodes, without changing the positions or subtrees of those nodes (can be thought of as flipping the sign in an operation node).

\section{Adversarial Search and Games}\label{sec:adversarial-search-and-games}
\subsection{Game Theory}\label{subsec:5.1}
I don't know what move my opponent will choose, but I am going to \emph{ASSUME} that it is going to be the best / optimal option.

\subsection{Optimal Decision in Games}\label{subsec:5.2}
\[ \Call{MinMax}{n} = \left\{ \begin{array}{c}
	\Call{Utility}{n, MAX},if\ \Call{Is-Terminal}{n}\\
	\max_{a\in\Call{Actions}{n}}\Call{MinMax}{Result(n,a)},\\if\ \Call{To-Move}{s} == MAX\\\\
	\min_{a\in\Call{Actions}{n}}\Call{MinMax}{Result(n,a)},\\if\ \Call{To-Move}{s} == MIN\\
\end{array} \right. \]
\subsection{Summary}\label{subsec:5-summary}
Go through the chapter summary.

\section{Constraint Satisfaction Problems}\label{sec:constraint-satisfaction-problems}
\subsection{Defining CSPs}\label{subsec:6.1}
You may be asked to formally define a constraint satisfaction problem.
\subsection{Constraint Propagation: Inference in CSPs}\label{subsec:6.2}
Minimum-remaining-values (MRV) heuristic: choose the variable with the ``fewest'' legal values
%
\subsection{Backtracking Search for CSPs}\label{subsec:6.3}
Ignore sections 6.3.3 and 6.3.4.
\subsection{Summary}\label{subsec:6-summary}
Go through the chapter summary.

\section{Ant Colony Optimization}\label{sec:ant-colony-optimization}

\definition{Stigmergy}{two individuals interact \emph{indirectly} when one of them modifies the environment and the other responds to the new environment at a later time}.

\subsection{Pheromone Trail}\label{subsec:pheromone-trail}
Individual ants deposit pheromones while travelling between nest and food source.
Pheromone trail \emph{gradually evaporates over time}.
Pheromone trail strength accumulates with multiple ants using the path.
%
\subsection{Artificial Ant Properties}\label{subsec:artificial-ant-properties}
\definition{Memory}{the list of visited places (nodes in the graph)}.
\definition{Best fitness}{the lowest cost total distance traveled between places}
\definition{Action}{select next location (node) to visit (and leave pheromones along the way)}
\definition{$\alpha$}{pheromone intensity weight}
\definition{$\beta$}{heuristic weight}

\[ P(move\ i\rightarrow j) = \frac{d_{ij}^{\alpha}\times\frac{1}{h_{ij}}^{\beta}}{\sum_{k=1}^{N\ possible\ destinations} d_{ik}^{\alpha}\times\frac{1}{h_{ik}}^{\beta}} \]

\begin{algorithm}[H]
	\caption{Simulated Annealing}\label{alg:simulated-annealing}
	\begin{algorithmic}[1]
		\Function{Simulated-Annealing}{$problem$, $Schedule$} \Returns a solution state
		\State $current\gets problem.INITIAL$
		\For{$t=1$ to $\infty$}
			\State $T\gets\Call{Schedule}{t}$
			\If{$T == 0$}
				\Return $current$
			\EndIf
			\State $next\gets$ a randomly selected successor of $current$
			\State $\Delta E\gets\Call{Value}{current} - \Call{Value}{next}$
			\If{$\Delta E > 0$}
				$current\gets next$
			\Else\
			$current\gets next$ only with probability $e^{\Delta E/T}$
			\EndIf
		\EndFor
		\EndFunction
	\end{algorithmic}
\end{algorithm}
%
\begin{algorithm}[H]
	\caption{Hill-climbing search}\label{alg:hill-climbing}
	\begin{algorithmic}[1]
		\Function{Hill-Climbing}{$problem$} \Returns a state that is a local maximum
		\State $current\gets problem.INITIAL$
		\While{true}
			\State $neighbor\gets$ a highest-valued successor state of $current$
			\If{$\Call{Value}{neighbor}\leq\Call{Value}{current}$}
				\Return $current$
			\EndIf
			\State $current\gets neighbor$
		\EndWhile
		\EndFunction
	\end{algorithmic}
\end{algorithm}
%
\begin{algorithm}[H]
	\caption{Best-First Search: A* Pseudocode}\label{alg:a*}
	\begin{algorithmic}[1]
		\Function{Best-First-Search}{$problem, f$} \Returns a solution node or \textit{failure}
		\State $\textcolor{nodeblue}{node}\gets\Call{Node}{State=problem.Initial}$
		\State $\textcolor{frontiergreen}{frontier}\gets$ a priority queue ordered by $f$, with $\textcolor{nodeblue}{node}$ as an element
		\State $\textcolor{reachedpurple}{reached}\gets$ a lookup table, with one entry with key $problem.\Call{Initial}{}$ and value $\textcolor{nodeblue}{node}$
		\While{\Not $\Call{Is-Empty}{\textcolor{frontiergreen}{frontier}}$}
			\State $\textcolor{nodeblue}{node}\gets\Call{Pop}{\textcolor{frontiergreen}{frontier}}$
			\If{$problem.\Call{Is-Goal}{\textcolor{nodeblue}{node}.State}$}
				\Return $\textcolor{nodeblue}{node}$
			\EndIf
			\ForAll{$\textcolor{childred}{child}$ \In $\Call{Expand}{problem,\ \textcolor{nodeblue}{node}}$}
				\State $\textcolor{stateorange}{s}\gets \textcolor{childred}{child}.\Call{State}{}$
				\If{$\textcolor{stateorange}{s}$ \Not \In $\textcolor{reachedpurple}{reached}$ or $child.\Call{Path-Cost}{} < \textcolor{reachedpurple}{reached}[\textcolor{stateorange}{s}].\Call{Path-Cost}{}$}
					\State $\textcolor{reachedpurple}{reached}[\textcolor{stateorange}{s}]\gets child$
					\State $\textcolor{childred}{child}$ to $\textcolor{frontiergreen}{frontier}$
				\EndIf
			\EndFor
		\EndWhile
		\State \Return $failure$
		\EndFunction
		\Function{Expand}{$problem, node$} yields nodes
		\State $s\gets node.\Call{State}{}$
		\ForAll{$action$ \In $problem.\Call{Actions}{s}$}
			\State $s'\gets problem.\Call{Result}{s, action}$
			\State \makebox{$cost\gets node.\Call{Path-Cost}{} + problem.\Call{Action{-}Cost}{s, action, s'}$}
			\State yield $\Call{Node}{State=s', Parent=node, Action=action, Path-Cost=cost}$
		\EndFor
		\EndFunction
	\end{algorithmic}
\end{algorithm}
%
\begin{algorithm}[H]
	\caption{Genetic Algorithm Pseudocode}\label{alg:genetric-algorithm}
	\begin{algorithmic}[1]
		\Function{Genetric-Algorithm}{$population,\ fitness$} \Returns an individual
		\Repeat
			\State $weights\gets\Call{Weighted-By}{population, fitness}$
			\State $population2\gets$ empty list
			\For{$i=1$ to $\Call{Size}{population}$}
				\State $parent1,\ parent2\gets\Call{Weighted-Random-Choices}{population, weights, 2}$
				\State $child\gets\Call{Reproduce}{parent1,\ parent2}$
				\If{small random probability}
					$child\gets\Call{Mutate}{child}$
				\EndIf
				\State add $child$ to $population2$
			\EndFor
			\State $population\gets population2$
		\Until{some individual is fit enough, or enough time has elapsed}
		\State \Return the best individual in $population$, according to $fitness$
		\EndFunction
		\Function{Reproduce}{$parent1,parent2$} \Returns an individual
		\State $n\gets\Call{Length}{parent1}$
		\State $c\gets$ random number from 1 to $n$
		\State \Return Append(\Call{Substring}{$parent1$, 1, $c$}, \Call{Substring}{$parent2$, $c+1$, $n$})
		\EndFunction
	\end{algorithmic}
\end{algorithm}

\end{document}