%! Author = Len Washington III
%! Date = 2/20/24

% Preamble
\documentclass[exam={Midterm},color=true]{cs581exam}

\ifcolor
\definecolor{nodeblue}{HTML}{00b0f0}
\definecolor{frontiergreen}{HTML}{00b050}
\definecolor{reachedpurple}{HTML}{8d5ab3}
\definecolor{childred}{HTML}{c00000}
\definecolor{stateorange}{HTML}{ffc000}
\else
\definecolor{nodeblue}{HTML}{000000}
\definecolor{frontiergreen}{HTML}{000000}
\definecolor{reachedpurple}{HTML}{000000}
\definecolor{childred}{HTML}{000000}
\definecolor{stateorange}{HTML}{000000}
\renewcommand{\emph}[1]{\textbf{#1}}
\fi

\setcounter{section}{1}

% Document
\begin{document}

\section{Intelligent Agents}\label{sec:intelligent-agents}
\subsection{Agents and Environments}\label{subsec:2.1}
\definition{Percept}{content/information that agent's sensors are perceiving / capturing currently}.
\definition{Percept Sequence}{a complete history of everything that agent has ever perceived}.
Action $<$-$>$ percept sequence \emph{mapping} IS the agent \emph{function}.
Agent \emph{function} describes agent \emph{behavior}.
Agent \emph{function} is an \emph{abstract concept}.
Agent \emph{program} implements agent \emph{function}.

\subsection{The Concept of Rationality}\label{subsec:2.2}
A \emph{rational agent} is one that acts to achieve the best outcome, or when there is uncertainty, the best expected outcome.
Rationality maximizes what is EXPECTED to happen.
Perfection maximizes what WILL happen.
%
\subsection{The Nature of Environments}\label{subsec:2.3}
PEAS -- \Peas{}erformance measure;
\pEas{}nvironment in which the agent will operate;
\peAs{}ctuators that the agent will use to affect the environment;
\peaS{}ensors.
\subsubsection{Properties}\label{subsubsec:properties}
Fully vs partially observable.
Single agent vs multiagent (comp vs. co-op).
Deterministic vs. nondeterministic (stochastic/random).
Episode vs. sequential.
Static vs. dynamic.
Discrete vs. continuous.
Known vs. unknown (to the agent).
%
\subsection{The Structure of Agents}\label{subsec:2.4}
You may be asked to pick the best agent type for some problem and justify
your answer.
%
\subsection{Summary}\label{subsec:2-summary}
Go through the chapter summary.

\section{Solving Problems by Search}\label{sec:solving-problems-by-search}
\subsection{Problem-Solving Agents}\label{subsec:3.1}
Be comfortable defining a search problem.
\subsection{Example Problems}\label{subsec:3.2}
\subsection{Search Algorithms \& Uniform Search Strategies}\label{subsec:3.3-4}
%\begin{table}[H]
%    \centering
%    \begin{threeparttable}
%		\caption{Uniformed Search Algorithms}
%		\label{tab:uniformed-search-algorithms}
%		\begin{tabular}{|l*{6}{P{0.04\textwidth}}|}
%			\hline
%			Criterion & Breadth-First & Uniform Cost & Depth-First & Depth-Limited & Iterative Deepening & Bidirectional (if applicable)\\
%			\hline
%			Complete? & Yes & Yes & No & No & Yes & Yes\\
%			\hline
%			Optimal cost? & Yes & Yes & No & No & Yes & Yes\\
%			\hline
%			Time & $O(b^{d})$ & $O(b^{1+C\lfloor C*/\epsilon\rfloor})$
%		\end{tabular}
%		\begin{tablenotes}
%			\small
%			\item
%		\end{tablenotes}
%	\end{threeparttable}
%\end{table}

Ignore sections 3.4.4 and 3.4.5 for the exam.
\setcounter{subsection}{4}%
\subsection{Informed (Heuristic) Search Strategies}\label{subsec:3.5}
Informed search relies on \emph{domain-specific knowledge / hints} that help locate the goal state.
$h(n)=h(\mbox{State n})=\mbox{relevant information about State } n$

You may be asked to solve a search problem by hand.
\subsection{Heuristic Functions}\label{subsec:3.6}
Straight-line heuristics is \emph{admissible}: it never overestimates the cost.
An \emph{admissible heuristics} is guaranteed to give you the optimal solution.
Every \emph{consistent} heuristics is \emph{admissible}, but not the other way around.
A dominating heuristic is a heuristic that estimates closer to the actual cost than another heuristic.

\subsection{A*}\label{subsec:a*}
The evaluation function for A* is $f(n) = g(\mbox{State}_{n}) + h(\mbox{State}_{n})$ where $g(n)$ is the path cost from the initial node to node $n$,
and $h(n)$ is the \emph{estimated cost} of the best path that continues from node $n$ to a goal node.
A node $n$ with minimum (or maximum if necessary) $f(n)$ should be chosen for expansion.
\begin{algorithm}[H]
	\caption{Best-First Search: A* Pseudocode}\label{alg:a*}
	\begin{algorithmic}[1]
		\Function{Best-First-Search}{$problem, f$} \Returns a solution node or \textit{failure}
			\State $\textcolor{nodeblue}{node}\gets\Call{Node}{State=problem.Initial}$
			\State $\textcolor{frontiergreen}{frontier}\gets$ a priority queue ordered by $f$, with $\textcolor{nodeblue}{node}$ as an element
			\State $\textcolor{reachedpurple}{reached}\gets$ a lookup table, with one entry with key $problem.\Call{Initial}{}$ and value $\textcolor{nodeblue}{node}$
			\While{\Not $\Call{Is-Empty}{\textcolor{frontiergreen}{frontier}}$}
				\State $\textcolor{nodeblue}{node}\gets\Call{Pop}{\textcolor{frontiergreen}{frontier}}$
				\If{$problem.\Call{Is-Goal}{\textcolor{nodeblue}{node}.State}$}
					\Return $\textcolor{nodeblue}{node}$
				\EndIf
				\ForAll{$\textcolor{childred}{child}$ \In $\Call{Expand}{problem,\ \textcolor{nodeblue}{node}}$}
					\State $\textcolor{stateorange}{s}\gets \textcolor{childred}{child}.\Call{State}{}$
					\If{$\textcolor{stateorange}{s}$ \Not \In $\textcolor{reachedpurple}{reached}$ or $child.\Call{Path-Cost}{} < \textcolor{reachedpurple}{reached}[\textcolor{stateorange}{s}].\Call{Path-Cost}{}$}
						\State $\textcolor{reachedpurple}{reached}[\textcolor{stateorange}{s}]\gets child$
						\State $\textcolor{childred}{child}$ to $\textcolor{frontiergreen}{frontier}$
					\EndIf
				\EndFor
			\EndWhile
			\State \Return $failure$
		\EndFunction
		\Function{Expand}{$problem, node$} yields nodes
			\State $s\gets node.\Call{State}{}$
			\ForAll{$action$ \In $problem.\Call{Actions}{s}$}
				\State $s'\gets problem.\Call{Result}{s, action}$
				\State \makebox{$cost\gets node.\Call{Path-Cost}{} + problem.\Call{Action{-}Cost}{s, action, s'}$}
				\State yield $\Call{Node}{State=s', Parent=node, Action=action, Path-Cost=cost}$
			\EndFor
		\EndFunction
	\end{algorithmic}
\end{algorithm}
\subsection{Summary}\label{subsec:3-summary}
Go through the chapter summary. FOCUS ON A* algorithm

\section{Search in Complex Environments}\label{sec:search-in-complex-environments}
\subsection{Local Search and Optimization Problems}\label{subsec:local-search-and-optimization-problems}
Local search doesn't care about the path to the goal, just getting to the goal.
They're useful for pure optimization problems (finding the best state according to an objective function.)
Generally use a single current state and generally move to neighbors of that state.
Two key advantages are: little memory usage (usually a constant amount) and can find reasonable solutions in large of infinite (continuous) states spaces.
The performance can be measured using \emph{completeness} (guaranteed to find a solution when there is one and report when there isn't), \emph{cost-optimality} (does it find a solution with the lowest path cost of all solutions), or time or space complexity.
%
\subsubsection{Hill-climbing search}\label{subsubsec:4.1.1}
The most primitive informed search approach; it is a naive greedy algorithm and the objective function is the value of the next state.
The agent can get stuck on peaks (local maxima), ridges (sequences of peaks), and plateaus (areas where the evaluation function has the same value).
\begin{algorithm}[H]
	\caption{Hill-climbing search}\label{alg:hill-climbing}
	\begin{algorithmic}[1]
	\Function{Hill-Climbing}{$problem$} \Returns a state that is a local maximum
		\State $current\gets problem.INITIAL$
		\While{true}
			\State $neighbor\gets$ a highest-valued successor state of $current$
			\If{$\Call{Value}{neighbor}\leq\Call{Value}{current}$}
				\Return $current$
			\EndIf
			\State $current\gets neighbor$
		\EndWhile
	\EndFunction
	\end{algorithmic}
\end{algorithm}

\subsubsection{Simulated Annealing}\label{subsubsec:4.1.2}
Accepts a move if it improves the objective value, and accepts some ``bad'' moves given some probability depending on the current objective value.
Converges to a global optimum; in practice, it can give excellent results.
\begin{algorithm}[H]
	\caption{Simulated Annealing}\label{alg:simulated-annealing}
	\begin{algorithmic}[1]
	\Function{Simulated-Annealing}{$problem$, $Schedule$} \Returns a solution state
		\State $current\gets problem.INITIAL$
		\For{$t=1$ to $\infty$}
			\State $T\gets\Call{Schedule}{t}$
			\If{$T == 0$}
				\Return $current$
			\EndIf
			\State $next\gets$ a randomly selected successor of $current$
			\State $\Delta E\gets\Call{Value}{current} - \Call{Value}{next}$
			\If{$\Delta E > 0$}
				$current\gets next$
			\Else\
				$current\gets next$ only with probability $e^{\Delta E/T}$
			\EndIf
		\EndFor
	\EndFunction
	\end{algorithmic}
\end{algorithm}

\setcounter{subsection}{3}%
\subsubsection{Evolutionary algorithms}\label{subsubsec:4.1.4}
\definition{Nature}{speciation occurs when two similar reproducing beings evolve to become too dissimilar to share genetic information effectively or correctly}.
\definition{Implementation}{speciation$\rightarrow$some mathematical function that established the similarity between two candidate solutions in the population}
\begin{algorithm}[H]
	\caption{Genetic Algorithm Pseudocode}\label{alg:genetric-algorithm}
	\begin{algorithmic}[1]
		\Function{Genetric-Algorithm}{$population,\ fitness$} \Returns an individual
		\Repeat
			\State $weights\gets\Call{Weighted-By}{population, fitness}$
			\State $population2\gets$ empty list
			\For{$i=1$ to $\Call{Size}{population}$}
				\State $parent1,\ parent2\gets\Call{Weighted-Random-Choices}{population, weights, 2}$
				\State $child\gets\Call{Reproduce}{parent1,\ parent2}$
				\If{small random probability}
					$child\gets\Call{Mutate}{child}$
				\EndIf
				\State add $child$ to $population2$
			\EndFor
			\State $population\gets population2$
		\Until{some individual is fit enough, or enough time has elapsed}
		\State \Return the best individual in $population$, according to $fitness$
		\EndFunction
		\Function{Reproduce}{$parent1,parent2$} \Returns an individual
			\State $n\gets\Call{Length}{parent1}$
			\State $c\gets$ random number from 1 to $n$
			\State \Return Append(\Call{Substring}{$parent1$, 1, $c$}, \Call{Substring}{$parent2$, $c+1$, $n$})
		\EndFunction
	\end{algorithmic}
\end{algorithm}

$\dots$and everything related to Evolutionary algorithms that I covered in class (especially: EVERYTHING about GENETIC ALGORITHM)\\
IGNORE TABU SEARCH

\subsubsection{Genetic Programming (GP)}\label{subsubsec:genetic-programming} % TODO: Agent States, Environment, ..., Game Theory (MinMax)
GP is an automated method for creating a working computer program from a high-level problem statement of a problem.
It starts from a high-level statement of ``what needs to be done'' and automatically creates a computer program to solve the problem.
Genotypes are trees, whereas the phenotypes (what can be seen) is the evaluation score.
Mutation occurs by picking a node for random mutation (by generating a new random subtree) and replacing the node with the root of the new subtree.
Crossover occurs by swapping the values of two nodes, without changing the positions or subtrees of those nodes (can be thought of as flipping the sign in an operation node).

\section{Adversarial Search and Games}\label{sec:adversarial-search-and-games}
\subsection{Game Theory}\label{subsec:5.1}
\subsection{Optimal Decision in Games}\label{subsec:5.2}
You may be asked to solve an adversarial problem by hand using Min-Max
and alpha-beta pruning. Ignore section 5.2.2.
\subsection{Summary}\label{subsec:5-summary}
Go through the chapter summary.

\section{Constraint Satisfaction Problems}\label{sec:constraint-satisfaction-problems}
\subsection{Defining CSPs}\label{subsec:6.1}
You may be asked to formally define a constraint satisfaction problem.
\subsection{Constraint Propagation: Inference in CSPs}\label{subsec:6.2}
Ignore sections 6.2.4 and 6.2.5.
\subsection{Backtracking Search for CSPs}\label{subsec:6.3}
Ignore sections 6.3.3 and 6.3.4.
\subsection{Summary}\label{subsec:6-summary}
Go through the chapter summary.

\section{Ant Colony Optimization}\label{sec:ant-colony-optimization}

\end{document}