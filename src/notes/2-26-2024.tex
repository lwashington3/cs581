%! Author = Len Washington III
%! Date = 2/26/2024
%! compiler = pdflatex

% Preamble
\documentclass[title={February 26, 2024 Notes}]{cs581notes}

% Document
\begin{document}

%<*2-26-2024>

\section{Gradients and Gradient Descent}\label{sec:gradient-descent}
\subsection{Gradient Descent Algorithm}\label{subsec:gradient-descent-algorithm}
\begin{itemize}
	\item Pick an initial point $x_{0}$
	\item Iterate until convergence \[ x_{t+1} = x_{t} - \gamma_{t}\nabla f(x_{t}) \] where $\gamma_{t}$ is the $t^{th}$ step size (sometimes called learning rate)
\end{itemize}

\section{Empirical Gradient}\label{sec:empirical-gradient}
In other cases, the objective / evaluation function might not be available in a differentiable form

\section{Gradients and Learning Rate / Step}\label{sec:gradients-and-learning-rate-/-step}
\begin{itemize}
	\item The value of the gradient
\end{itemize}

\chapter{Searching in Partially Obserable and Nondeterministic Environments}\label{ch:searching-in-partially-obserable-and-nondeterministic-environments}

%</2-26-2024>

\end{document}